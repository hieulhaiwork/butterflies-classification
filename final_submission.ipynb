{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name: Le Hien Hieu\n",
    "### ID: HE181040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U datasets evaluate albumentations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoFeatureExtractor, ViTForImageClassification, SwinForImageClassification\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /content/original_data\n",
    "!unzip -q /content/drive/MyDrive/dat-301-m-ai-1802-ads-butterfly-classification.zip -d /content/original_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show number of images in train folder\n",
    "print(\"Number of images in train folder: \", len(os.listdir(\"/content/original_data/train/train\")))\n",
    "\n",
    "# Show number of images in test folder\n",
    "print(\"Number of images in test folder: \", len(os.listdir(\"/content/original_data/test/test\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split2folder(path: os.PathLike, saved_path: os.PathLike, df: pd.DataFrame):\n",
    "  \"\"\"\n",
    "  Read labels from a dataframe and save images to corresponding folders.\n",
    "\n",
    "  Args:\n",
    "    path (os.PathLike): path to the folder containing images.\n",
    "    saved_path (os.PathLike): path to save images.\n",
    "    df (pd.DataFrame): dataframe containing image names and labels\n",
    "  \"\"\"\n",
    "\n",
    "  if not os.path.exists(saved_path):\n",
    "    os.makedirs(saved_path)\n",
    "\n",
    "  for i in range(len(df)):\n",
    "    img_name = df.iloc[i, 0]\n",
    "    label = df.iloc[i, 1].upper()\n",
    "    img = Image.open(path + img_name)\n",
    "\n",
    "    if not os.path.exists(saved_path + label):\n",
    "      os.makedirs(saved_path + label)\n",
    "\n",
    "    img.save(saved_path + label + \"/\" + img_name)\n",
    "\n",
    "  print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df = pd.read_csv(\"/content/original_data/Training_set.csv\")\n",
    "image_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA on image_df\n",
    "image_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of each label and plot the distribution\n",
    "label_count = image_df[\"label\"].value_counts()\n",
    "label_count.plot(kind=\"bar\", figsize=(10, 5))\n",
    "plt.title(\"Distribution of labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split2folder(path=\"/content/original_data/train/train/\", saved_path=\"/content/train_dataset/\", df=image_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define\n",
    "model_name_1 = \"google/vit-base-patch16-224-in21k\"\n",
    "model_name_2 = \"microsoft/swin-base-patch4-window7-224\"\n",
    "\n",
    "root_dir = \"/content/train_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset based on Huggingface format dataset\n",
    "ds = load_dataset(\"imagefolder\", data_dir=root_dir)\n",
    "\n",
    "# Split the dataset into train and validation\n",
    "train_valid_dataset = ds['train'].train_test_split(test_size=0.2, shuffle=True)\n",
    "validation_dataset = train_valid_dataset['test']\n",
    "train_dataset = train_valid_dataset['train']\n",
    "\n",
    "train_dataset, validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display an image from the dataset and label\n",
    "im = train_dataset[0]['image']\n",
    "label = train_dataset[0]['label']\n",
    "display(im)\n",
    "print(\"Label: \", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation\n",
    "def train_augment(example: dict):\n",
    "    \"\"\"\n",
    "    Apply augmentation for single image.\n",
    "    Args:\n",
    "        example (dict): dictionary containing image and label.\n",
    "    \"\"\"\n",
    "    image = example['image'] # PIL format\n",
    "\n",
    "    # Convert PIL image to numpy array\n",
    "    image = np.asarray(image, dtype=np.uint8) \n",
    "\n",
    "    aug = A.Compose([\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.2),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.Rotate(limit=30, interpolation=cv2.INTER_CUBIC, p=0.5),\n",
    "        A.CoarseDropout(num_holes_range=(1, 5),\n",
    "                        hole_height_range=(32, 64),\n",
    "                        hole_width_range=(32, 64),\n",
    "                        fill=0,\n",
    "                        p=0.5),\n",
    "    ])\n",
    "\n",
    "    transform = aug(image=image)\n",
    "\n",
    "    return {\n",
    "        \"image\": Image.fromarray(transform['image']),\n",
    "        \"label\": example[\"label\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class for applying augmentation.\n",
    "    Args:\n",
    "        dataset (Dataset): dataset containing images and labels.\n",
    "        augment_fn (callable): augmentation function.\n",
    "        processor (AutoFeatureExtractor): processor to preprocess\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, augment_fn, processor):\n",
    "        self.dataset = dataset\n",
    "        self.augment_fn = augment_fn\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.dataset[idx]\n",
    "        augmented_example = self.augment_fn(example)\n",
    "\n",
    "        augmented_image = augmented_example['image']\n",
    "        label = augmented_example['label']\n",
    "\n",
    "        processed_inputs = self.processor(images=augmented_image, return_tensors='pt')\n",
    "\n",
    "        inputs = {\n",
    "            'pixel_values': processed_inputs.pixel_values.squeeze(),\n",
    "            'labels': label\n",
    "        }\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create processor\n",
    "processor_1 = AutoFeatureExtractor.from_pretrained(model_name_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_1(example_batch):\n",
    "    \"\"\"\n",
    "    Apply processor to a batch of images.\n",
    "    Args:\n",
    "        example_batch (dict): batch of images and labels.\n",
    "        processor (AutoFeatureExtractor): processor to preprocess.\n",
    "    \"\"\"\n",
    "    inputs = processor_1([x for x in example_batch['image']],return_tensors='pt')\n",
    "    inputs['labels'] = example_batch['label']\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformed validation dataset \n",
    "# No need to augment validation dataset but only turn it into tensor\n",
    "tf_validation_dataset = validation_dataset.with_transform(transform_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first element of the transformed validation dataset to check\n",
    "tf_validation_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformed train dataset: apply augmentation and turn it into tensor\n",
    "tf_train_dataset = AugmentedDataset(train_dataset, train_augment, processor=processor_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first element of the transformed train dataset to check\n",
    "tf_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch: list):\n",
    "    \"\"\"\n",
    "    Collate function to be used in DataLoader.\"\n",
    "    Args:\n",
    "        batch (list): list of examples.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['labels'] for x in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metrics for evaluation\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \"\"\"\n",
    "    Compute metrics for evaluation, assume labels are in the form of integers.\n",
    "    Args:\n",
    "        p (Trainer): trainer object containing predictions and labels.\n",
    "    \"\"\"\n",
    "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for labels\n",
    "labels = tf_validation_dataset.features['label'].names\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name_1,\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)},\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./vit-butterflies-google-final\",\n",
    "  per_device_train_batch_size=32,\n",
    "  eval_strategy=\"steps\",\n",
    "  num_train_epochs=50,\n",
    "  fp16=True,\n",
    "  save_steps=500,\n",
    "  eval_steps=500,\n",
    "  logging_steps=10,\n",
    "  learning_rate=2e-4,\n",
    "  save_total_limit=5,\n",
    "  remove_unused_columns=False,\n",
    "  push_to_hub=True,\n",
    "  report_to='tensorboard',\n",
    "  load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=tf_train_dataset,\n",
    "    eval_dataset=tf_validation_dataset,\n",
    "    processing_class=processor_1,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create processor 2\n",
    "processor_2 = AutoFeatureExtractor.from_pretrained(model_name_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_2(example_batch):\n",
    "    \"\"\"\n",
    "    Apply processor to a batch of images.\n",
    "    Args:\n",
    "        example_batch (dict): batch of images and labels.\n",
    "        processor (AutoFeatureExtractor): processor to preprocess.\n",
    "    \"\"\"\n",
    "    inputs = processor_2([x for x in example_batch['image']],return_tensors='pt')\n",
    "    inputs['labels'] = example_batch['label']\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformed train dataset: apply augmentation and turn it into tensor\n",
    "tf_validation_dataset = validation_dataset.with_transform(transform_2)\n",
    "\n",
    "# Create transformed train dataset: apply augmentation and turn it into tensor\n",
    "tf_train_dataset = AugmentedDataset(train_dataset, train_augment, processor=processor_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutmix = transforms.v2.CutMix(num_classes=75)\n",
    "mixup = transforms.v2.MixUp(num_classes=75)\n",
    "cutmix_or_mixup = transforms.v2.RandomChoice([cutmix, mixup])\n",
    "\n",
    "# Create collator applying cutmix or mixup\n",
    "class CutMixOrMixUpCollator:\n",
    "    \"\"\"\n",
    "    Custom collator to apply cutmix or mixup.\n",
    "    Args:\n",
    "        collator (callable): collator function to be used.\n",
    "        cutmix_or_mixup (callable): cutmix or mixup function to be used.\n",
    "    \"\"\"\n",
    "    def __init__(self, collator, cutmix_or_mixup):\n",
    "        self.collator = collator\n",
    "        self.cutmix_or_mixup = cutmix_or_mixup\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        # Collate the examples\n",
    "        batch = self.collator(examples)\n",
    "\n",
    "        pixel_values = batch[\"pixel_values\"]\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        if pixel_values.ndim == 4: # Check if the input is a batch of images\n",
    "            augmented_batch = self.cutmix_or_mixup(pixel_values, labels)\n",
    "            batch[\"pixel_values\"] = augmented_batch[0]\n",
    "            batch[\"labels\"] = augmented_batch[1]\n",
    "\n",
    "        return batch\n",
    "\n",
    "cutmix_mixup_collector = CutMixOrMixUpCollator(collate_fn, cutmix_or_mixup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \"\"\"\n",
    "    Compute metrics for evaluation, cutmix and mixup return one-hot encoded labels.\n",
    "    Args:\n",
    "        p (Trainer): trainer object containing predictions and labels.\n",
    "    \"\"\"\n",
    "    hard_labels = np.argmax(p.label_ids, axis=1)\n",
    "    predictions = np.argmax(p.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=hard_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SwinForImageClassification.from_pretrained(\n",
    "    model_name_2,\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)},\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze embeddings layer and encoder layers 0, 1, 2\n",
    "for name, param in model.named_parameters():\n",
    "    if \"swin.embeddings\" in name:\n",
    "        param.requires_grad = False\n",
    "    elif \"swin.encoder.layers.0\" in name:\n",
    "        param.requires_grad = False\n",
    "    elif \"swin.encoder.layers.1\" in name:\n",
    "        param.requires_grad = False\n",
    "    elif \"swin.encoder.layers.2\" in name:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_params = 0\n",
    "total_params = 0\n",
    "for param in model.parameters():\n",
    "    total_params += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "print(f\"Freezen parameters: {total_params - trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = model.config.hidden_size\n",
    "new_classifier = nn.Sequential(\n",
    "    nn.Linear(hidden_size, hidden_size // 2),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(hidden_size // 2, hidden_size // 4),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(hidden_size // 4, len(labels))\n",
    ")\n",
    "model.classifier = new_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./result/swin-base-patch4-window7-224\",\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=32,\n",
    "    learning_rate=1e-4,\n",
    "\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    per_device_eval_batch_size = 32,\n",
    "\n",
    "    fp16=True,\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    logging_steps=10,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=True,\n",
    "    report_to='tensorboard',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=cutmix_mixup_collector,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=tf_train_dataset,\n",
    "    eval_dataset=tf_validation_dataset,\n",
    "    processing_class=processor_2,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolderDataset(Dataset):\n",
    "    def __init__(self, image_folder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.image_paths = [os.path.join(image_folder, img) for img in os.listdir(image_folder) if img.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))] # Lọc các file ảnh\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, image_path\n",
    "\n",
    "# Transform cho Swin Transformer\n",
    "image_size = 224 \n",
    "mean = (0.485, 0.456, 0.406) # Mean ImageNet\n",
    "std = (0.229, 0.224, 0.225)  # Std ImageNet\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model đã train (hoặc pre-trained nếu bạn dùng pre-trained)\n",
    "model_name_1 = \"hieulhwork24/vit-butterflies-google-final\"\n",
    "model_name_2 = \"hieulhwork24/swinv2-base-patch4-window8-256\"\n",
    "\n",
    "model_1 = ViTForImageClassification.from_pretrained(model_name_1, num_labels=75)\n",
    "model_2 = SwinForImageClassification.from_pretrained(model_name_2, num_labels=75)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_1.to(device)\n",
    "model_2.to(device)\n",
    "model_1.eval()\n",
    "model_2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folder = \"/content/test\"\n",
    "batch_size = 256\n",
    "num_workers = 4\n",
    "\n",
    "test_dataset = ImageFolderDataset(test_folder, transform=transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "predictions = {}\n",
    "class_names = sorted(os.listdir(\"/content/train_2\"))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_images, image_paths in test_dataloader:\n",
    "        batch_images = batch_images.to(device)\n",
    "\n",
    "        outputs_1 = model_1(batch_images)\n",
    "        logits_1 = outputs_1.logits\n",
    "\n",
    "        outputs_2 = model_2(batch_images)\n",
    "        logits_2 = outputs_2.logits\n",
    "\n",
    "        # Calculate avaerage probablities of class then choose max label\n",
    "        average_logits = (logits_1 + logits_2) / 2\n",
    "        predicted_classes = torch.argmax(average_logits, dim=-1)\n",
    "\n",
    "        for i in range(len(image_paths)):\n",
    "            image_path = image_paths[i]\n",
    "            predicted_class_index = predicted_classes[i].item()\n",
    "            predicted_class_name = class_names[predicted_class_index]\n",
    "            predictions[os.path.basename(image_path)] = predicted_class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predition_df = pd.DataFrame(list(predictions.items()), columns=[\"ID\", \"label\"])\n",
    "\n",
    "predition_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
